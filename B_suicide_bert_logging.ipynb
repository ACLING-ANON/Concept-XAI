{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\salma\\anaconda3\\envs\\env-gpu-10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.datasets import IMDB\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import dataset, TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from itertools import *\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "from captum.concept import TCAV\n",
    "from captum.concept import Concept\n",
    "from captum.concept._utils.common import concepts_to_str\n",
    "import matplotlib.pyplot as plt\n",
    "from captum.attr._core.layer.layer_activation import LayerActivation \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding dimension 768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TextClassificationModel(768, 2)\n",
    "model.load_state_dict(torch.load(f\"models/bert_suicide_BCELoss_27June.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging Attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextClassificationModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=768, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=8, bias=True)\n",
       "  (fc4): Linear(in_features=8, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1model.bert\n",
      "2model.bert.embeddings\n",
      "3model.bert.embeddings.word_embeddings\n",
      "3model.bert.embeddings.position_embeddings\n",
      "3model.bert.embeddings.token_type_embeddings\n",
      "3model.bert.embeddings.LayerNorm\n",
      "3model.bert.embeddings.dropout\n",
      "2model.bert.encoder\n",
      "3model.bert.encoder.layer\n",
      "4model.bert.encoder.layer.0\n",
      "5model.bert.encoder.layer.0.attention\n",
      "5model.bert.encoder.layer.0.intermediate\n",
      "5model.bert.encoder.layer.0.output\n",
      "4model.bert.encoder.layer.1\n",
      "5model.bert.encoder.layer.1.attention\n",
      "5model.bert.encoder.layer.1.intermediate\n",
      "5model.bert.encoder.layer.1.output\n",
      "4model.bert.encoder.layer.2\n",
      "5model.bert.encoder.layer.2.attention\n",
      "5model.bert.encoder.layer.2.intermediate\n",
      "5model.bert.encoder.layer.2.output\n",
      "4model.bert.encoder.layer.3\n",
      "5model.bert.encoder.layer.3.attention\n",
      "5model.bert.encoder.layer.3.intermediate\n",
      "5model.bert.encoder.layer.3.output\n",
      "4model.bert.encoder.layer.4\n",
      "5model.bert.encoder.layer.4.attention\n",
      "5model.bert.encoder.layer.4.intermediate\n",
      "5model.bert.encoder.layer.4.output\n",
      "4model.bert.encoder.layer.5\n",
      "5model.bert.encoder.layer.5.attention\n",
      "5model.bert.encoder.layer.5.intermediate\n",
      "5model.bert.encoder.layer.5.output\n",
      "4model.bert.encoder.layer.6\n",
      "5model.bert.encoder.layer.6.attention\n",
      "5model.bert.encoder.layer.6.intermediate\n",
      "5model.bert.encoder.layer.6.output\n",
      "4model.bert.encoder.layer.7\n",
      "5model.bert.encoder.layer.7.attention\n",
      "5model.bert.encoder.layer.7.intermediate\n",
      "5model.bert.encoder.layer.7.output\n",
      "4model.bert.encoder.layer.8\n",
      "5model.bert.encoder.layer.8.attention\n",
      "5model.bert.encoder.layer.8.intermediate\n",
      "5model.bert.encoder.layer.8.output\n",
      "4model.bert.encoder.layer.9\n",
      "5model.bert.encoder.layer.9.attention\n",
      "5model.bert.encoder.layer.9.intermediate\n",
      "5model.bert.encoder.layer.9.output\n",
      "4model.bert.encoder.layer.10\n",
      "5model.bert.encoder.layer.10.attention\n",
      "5model.bert.encoder.layer.10.intermediate\n",
      "5model.bert.encoder.layer.10.output\n",
      "4model.bert.encoder.layer.11\n",
      "5model.bert.encoder.layer.11.attention\n",
      "5model.bert.encoder.layer.11.intermediate\n",
      "5model.bert.encoder.layer.11.output\n",
      "2model.bert.pooler\n",
      "3model.bert.pooler.dense\n",
      "3model.bert.pooler.activation\n",
      "1model.fc1\n",
      "1model.fc2\n",
      "1model.fc3\n",
      "1model.fc4\n"
     ]
    }
   ],
   "source": [
    "for x in model._modules:\n",
    "    print(f\"1model.{x}\")\n",
    "    for y in model.__getattr__(x)._modules:\n",
    "        print(f\"2model.{x}.{y}\")\n",
    "        for z in model.__getattr__(x).__getattr__(y)._modules:\n",
    "            print(f\"3model.{x}.{y}.{z}\")\n",
    "            for w in model.__getattr__(x).__getattr__(y).__getattr__(z)._modules:\n",
    "                print(f\"4model.{x}.{y}.{z}.{w}\")\n",
    "                for m in model.__getattr__(x).__getattr__(y).__getattr__(z).__getattr__(w)._modules:\n",
    "                    print(f\"5model.{x}.{y}.{z}.{w}.{m}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_layers = {}\n",
    "for x in model._modules:\n",
    "    level=1\n",
    "    for y in model.__getattr__(x)._modules:\n",
    "        level=2\n",
    "        for z in model.__getattr__(x).__getattr__(y)._modules:\n",
    "            level=3\n",
    "            for w in model.__getattr__(x).__getattr__(y).__getattr__(z)._modules:\n",
    "                level=4\n",
    "                # for m in model.__getattr__(x).__getattr__(y).__getattr__(z).__getattr__(w)._modules:\n",
    "                #     level=5 \n",
    "                #     if level==5:\n",
    "                #         all_layers[f\"model.{x}.{y}.{z}.{w}.{m}\"]=(model.__getattr__(x).__getattr__(y).__getattr__(z).__getattr__(w).__getattr__(m))\n",
    "                if level==4:\n",
    "                    all_layers[f\"model.{x}.{y}.{z}.{w}\"]=(model.__getattr__(x).__getattr__(y).__getattr__(z).__getattr__(w))\n",
    "            if level==3:\n",
    "                all_layers[f\"model.{x}.{y}.{z}\"]=(model.__getattr__(x).__getattr__(y).__getattr__(z))\n",
    "        if level==2:\n",
    "            all_layers[f\"model.{x}.{y}\"]=(model.__getattr__(x).__getattr__(y))\n",
    "    if level==1:\n",
    "        all_layers[f\"model.{x}\"]=model.__getattr__(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model.bert.embeddings.word_embeddings': Embedding(30522, 768, padding_idx=0),\n",
       " 'model.bert.embeddings.position_embeddings': Embedding(512, 768),\n",
       " 'model.bert.embeddings.token_type_embeddings': Embedding(2, 768),\n",
       " 'model.bert.embeddings.LayerNorm': LayerNorm((768,), eps=1e-12, elementwise_affine=True),\n",
       " 'model.bert.embeddings.dropout': Dropout(p=0.1, inplace=False),\n",
       " 'model.bert.encoder.layer.0': BertLayer(\n",
       "   (attention): BertAttention(\n",
       "     (self): BertSelfAttention(\n",
       "       (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (output): BertSelfOutput(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (intermediate): BertIntermediate(\n",
       "     (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "     (intermediate_act_fn): GELUActivation()\n",
       "   )\n",
       "   (output): BertOutput(\n",
       "     (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " 'model.bert.encoder.layer.1': BertLayer(\n",
       "   (attention): BertAttention(\n",
       "     (self): BertSelfAttention(\n",
       "       (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (output): BertSelfOutput(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (intermediate): BertIntermediate(\n",
       "     (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "     (intermediate_act_fn): GELUActivation()\n",
       "   )\n",
       "   (output): BertOutput(\n",
       "     (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " 'model.bert.encoder.layer.2': BertLayer(\n",
       "   (attention): BertAttention(\n",
       "     (self): BertSelfAttention(\n",
       "       (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (output): BertSelfOutput(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (intermediate): BertIntermediate(\n",
       "     (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "     (intermediate_act_fn): GELUActivation()\n",
       "   )\n",
       "   (output): BertOutput(\n",
       "     (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " 'model.bert.encoder.layer.3': BertLayer(\n",
       "   (attention): BertAttention(\n",
       "     (self): BertSelfAttention(\n",
       "       (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (output): BertSelfOutput(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (intermediate): BertIntermediate(\n",
       "     (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "     (intermediate_act_fn): GELUActivation()\n",
       "   )\n",
       "   (output): BertOutput(\n",
       "     (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " 'model.bert.encoder.layer.4': BertLayer(\n",
       "   (attention): BertAttention(\n",
       "     (self): BertSelfAttention(\n",
       "       (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (output): BertSelfOutput(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (intermediate): BertIntermediate(\n",
       "     (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "     (intermediate_act_fn): GELUActivation()\n",
       "   )\n",
       "   (output): BertOutput(\n",
       "     (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " 'model.bert.encoder.layer.5': BertLayer(\n",
       "   (attention): BertAttention(\n",
       "     (self): BertSelfAttention(\n",
       "       (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (output): BertSelfOutput(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (intermediate): BertIntermediate(\n",
       "     (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "     (intermediate_act_fn): GELUActivation()\n",
       "   )\n",
       "   (output): BertOutput(\n",
       "     (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " 'model.bert.encoder.layer.6': BertLayer(\n",
       "   (attention): BertAttention(\n",
       "     (self): BertSelfAttention(\n",
       "       (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (output): BertSelfOutput(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (intermediate): BertIntermediate(\n",
       "     (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "     (intermediate_act_fn): GELUActivation()\n",
       "   )\n",
       "   (output): BertOutput(\n",
       "     (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " 'model.bert.encoder.layer.7': BertLayer(\n",
       "   (attention): BertAttention(\n",
       "     (self): BertSelfAttention(\n",
       "       (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (output): BertSelfOutput(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (intermediate): BertIntermediate(\n",
       "     (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "     (intermediate_act_fn): GELUActivation()\n",
       "   )\n",
       "   (output): BertOutput(\n",
       "     (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " 'model.bert.encoder.layer.8': BertLayer(\n",
       "   (attention): BertAttention(\n",
       "     (self): BertSelfAttention(\n",
       "       (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (output): BertSelfOutput(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (intermediate): BertIntermediate(\n",
       "     (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "     (intermediate_act_fn): GELUActivation()\n",
       "   )\n",
       "   (output): BertOutput(\n",
       "     (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " 'model.bert.encoder.layer.9': BertLayer(\n",
       "   (attention): BertAttention(\n",
       "     (self): BertSelfAttention(\n",
       "       (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (output): BertSelfOutput(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (intermediate): BertIntermediate(\n",
       "     (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "     (intermediate_act_fn): GELUActivation()\n",
       "   )\n",
       "   (output): BertOutput(\n",
       "     (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " 'model.bert.encoder.layer.10': BertLayer(\n",
       "   (attention): BertAttention(\n",
       "     (self): BertSelfAttention(\n",
       "       (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (output): BertSelfOutput(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (intermediate): BertIntermediate(\n",
       "     (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "     (intermediate_act_fn): GELUActivation()\n",
       "   )\n",
       "   (output): BertOutput(\n",
       "     (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " 'model.bert.encoder.layer.11': BertLayer(\n",
       "   (attention): BertAttention(\n",
       "     (self): BertSelfAttention(\n",
       "       (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (output): BertSelfOutput(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (intermediate): BertIntermediate(\n",
       "     (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "     (intermediate_act_fn): GELUActivation()\n",
       "   )\n",
       "   (output): BertOutput(\n",
       "     (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " 'model.bert.pooler.dense': Linear(in_features=768, out_features=768, bias=True),\n",
       " 'model.bert.pooler.activation': Tanh(),\n",
       " 'model.fc1': Linear(in_features=768, out_features=64, bias=True),\n",
       " 'model.fc2': Linear(in_features=64, out_features=32, bias=True),\n",
       " 'model.fc3': Linear(in_features=32, out_features=8, bias=True),\n",
       " 'model.fc4': Linear(in_features=8, out_features=2, bias=True)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(all_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(all_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model.bert.embeddings.word_embeddings', 'model.bert.embeddings.position_embeddings', 'model.bert.embeddings.token_type_embeddings', 'model.bert.embeddings.LayerNorm', 'model.bert.embeddings.dropout', 'model.bert.encoder.layer.0', 'model.bert.encoder.layer.1', 'model.bert.encoder.layer.2', 'model.bert.encoder.layer.3', 'model.bert.encoder.layer.4', 'model.bert.encoder.layer.5', 'model.bert.encoder.layer.6', 'model.bert.encoder.layer.7', 'model.bert.encoder.layer.8', 'model.bert.encoder.layer.9', 'model.bert.encoder.layer.10', 'model.bert.encoder.layer.11', 'model.bert.pooler.dense', 'model.bert.pooler.activation', 'model.fc1', 'model.fc2', 'model.fc3', 'model.fc4'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_layers.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(attribution))\n",
    "# for (l,x) in zip(list(all_layers.keys()),attribution[0]):\n",
    "#     print(l,\"   \",x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "model.bert.embeddings.word_embeddings     torch.Size([5, 512, 768]) \\\n",
    "model.bert.embeddings.position_embeddings     torch.Size([1, 512, 768]) \\\n",
    "model.bert.embeddings.token_type_embeddings     torch.Size([5, 512, 768]) \\\n",
    "model.bert.embeddings.LayerNorm     torch.Size([5, 512, 768]) \\\n",
    "model.bert.embeddings.dropout     torch.Size([5, 512, 768]) \\\n",
    "model.bert.encoder.layer.0     torch.Size([5, 512, 768]) \\\n",
    "model.bert.encoder.layer.1     torch.Size([5, 512, 768]) \\\n",
    "model.bert.encoder.layer.2     torch.Size([5, 512, 768]) \\\n",
    "model.bert.encoder.layer.3     torch.Size([5, 512, 768]) \\\n",
    "model.bert.encoder.layer.4     torch.Size([5, 512, 768]) \\\n",
    "model.bert.encoder.layer.5     torch.Size([5, 512, 768]) \\\n",
    "model.bert.encoder.layer.6     torch.Size([5, 512, 768]) \\\n",
    "model.bert.encoder.layer.7     torch.Size([5, 512, 768]) \\\n",
    "model.bert.encoder.layer.8     torch.Size([5, 512, 768]) \\\n",
    "model.bert.encoder.layer.9     torch.Size([5, 512, 768]) \\\n",
    "model.bert.encoder.layer.10     torch.Size([5, 512, 768]) \\\n",
    "model.bert.encoder.layer.11     torch.Size([5, 512, 768]) \\\n",
    "model.bert.pooler.dense     torch.Size([5, 768]) \\\n",
    "model.bert.pooler.activation     torch.Size([5, 768]) \\\n",
    "model.fc1     torch.Size([5, 64]) \\\n",
    "model.fc2     torch.Size([5, 32]) \\\n",
    "model.fc3     torch.Size([5, 8]) \\\n",
    "model.fc4     torch.Size([5, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ex Wife Threatening SuicideRecently I left my ...</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Am I weird I don't get affected by compliments...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        class\n",
       "2  Ex Wife Threatening SuicideRecently I left my ...      suicide\n",
       "3  Am I weird I don't get affected by compliments...  non-suicide"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"suicide/Suicide_Detection.csv\", index_col=0)\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ex Wife Threatening SuicideRecently I left my ...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Am I weird I don't get affected by compliments...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Finally 2020 is almost over... So I can never ...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        class  label\n",
       "2  Ex Wife Threatening SuicideRecently I left my ...      suicide      1\n",
       "3  Am I weird I don't get affected by compliments...  non-suicide      0\n",
       "4  Finally 2020 is almost over... So I can never ...  non-suicide      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'] = (data['class']=='suicide').astype('Int64')\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((185659,), (185659,), (46415,), (46415,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X= np.array(data['text'])\n",
    "y = np.array(data['label'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y ,test_size=0.2, shuffle=True, stratify= y, random_state=42)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', truncation=True, max_length=512)\n",
    "def bert_tokenizer(text: str) -> Tuple[Tensor, Tensor]:\n",
    "    tokens = tokenizer.__call__(text, return_tensors='pt', padding='max_length', max_length=512, truncation=True)\n",
    "    words = tokenizer.convert_ids_to_tokens(tokens['input_ids'][0])\n",
    "\n",
    "    return tokens['input_ids'], tokens['attention_mask'], words\n",
    "def encode_labels(labels) -> Tensor:\n",
    "    return F.one_hot(torch.tensor([y for y in labels])).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_neg = X_train[np.invert(y_train.astype(bool))]\n",
    "X_train_pos = X_train[y_train.astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['normalise watching dubbed anime i don’t care if it’s not the original audio i prefer it',\n",
       "       'sad gorl hours I have school in a few minutes and I feel like fuckin shet bro\\n\\n\\nI want to cuddle with someone so bad it hurts',\n",
       "       'THE REDDIT APP ICON IS GOOD AGAIN It looks so weird seeing it with color again',\n",
       "       ...,\n",
       "       'Is posting a photo of me edited into an old photo with my dad karma whoring? I want to know if I should delete it. Recently posted a photo of my dad and a younger me in which I edited myself into it to have an updated photo because I can take one with him anymore. I simply posted it because it made me happy, but I want to know if that’s karma whoring because I know people have posted before purely just to get karma and awards which is horrible. Just want to get an idea if I should delete the post or not because I don’t want to be a karma whore.',\n",
       "       'I just took a online test And i got 25% because some of the question teacher needs to grade and i nsome answers i frogot put ? (It means like 8⁸ for exanple you know cuz u cant write like with pen on conputers) and or a forgot ro put - stuff like that irl in class teacher cares about those but she doesnt mark it wrong',\n",
       "       'Well, When Life Gives You Lemons - Vine Remix [https://www.youtube.com/watch?v=iv57akttTsI](https://www.youtube.com/watch?v=iv57akttTsI)'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 512]) torch.Size([5000, 512])\n",
      "torch.Size([5000, 512]) torch.Size([5000, 512])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def prepare_no_labels(data,start_idx=None, end_idx=None):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for t in data[start_idx:end_idx]:\n",
    "        tokens = bert_tokenizer(t)\n",
    "        input_ids.append(tokens[0])\n",
    "        attention_masks.append(tokens[1])\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    print(input_ids.shape, attention_masks.shape)\n",
    "    return torch.utils.data.TensorDataset(input_ids.to(device), attention_masks.to(device))\n",
    "\n",
    "train_dataset_neg =prepare_no_labels(X_train_neg, 0, 5000)\n",
    "train_dataset_pos =prepare_no_labels(X_train_pos, 0, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  101,  2296,  2210,  3291,  3084,  2033,  2215,  2000,  3102,  2870,\n",
       "          1012,  4714, 11809,  4485,  3957,  2033,  2200,  2172,  3255,  1012,\n",
       "          1045,  2215,  2000,  3102,  2870,  2138,  2026,  2482,  2003,  3714,\n",
       "          1010,  1045,  2215,  2000,  3102,  2870,  2738,  2084,  4550,  2026,\n",
       "          2282,  1010,  1045,  2215,  2000,  3102,  2870,  7188,  1045,  2031,\n",
       "          2000,  2079,  2070,  2524,  4485,  2030,  1045,  4558,  2030,  3338,\n",
       "          2242,  1012,  2023,  2003,  2035,  3722,  4485,  2008,  6433,  2000,\n",
       "          3071,  2021,  2035,  2023,  2785,  1997, 14636,  2064,  3426,  2200,\n",
       "          2172,  3255,  2000,  2033,  1012,  1998,  2045,  1005,  1055,  2053,\n",
       "          2126,  2000,  2644,  2383,  2023,  2785,  1997,  4485,  2144,  2166,\n",
       "          2003,  2440,  1997,  2210,  3471,  2005,  3071,  1012,  1045,  2031,\n",
       "          2060,  1010,  7046,  4436,  2005,  5782,  2000,  3280,  2029,  1045,\n",
       "          2453,  2191,  2488,  8466,  2055,  2021,  2023,  2001,  2074,  1037,\n",
       "          2210,  2518,  2008,  6616,  2015,  2033,  2039,  2200,  2172,  1012,\n",
       "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0], device='cuda:0'),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_pos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del data\n",
    "del X_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataloader_neg = DataLoader(\n",
    "            train_dataset_neg,  # The training samples.\n",
    "            sampler = None,# SequentialSampler(train_dataset), \n",
    "            batch_size = batch_size, # Trains with this batch size.\n",
    "            shuffle=False\n",
    "            # num_workers= 4\n",
    "        )\n",
    "\n",
    "train_dataloader_pos = DataLoader(\n",
    "            train_dataset_pos,  # The training samples.\n",
    "            sampler = None,# SequentialSampler(train_dataset), \n",
    "            batch_size = batch_size, # Trains with this batch size.\n",
    "            shuffle=False\n",
    "            # num_workers= 4\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved attribution for batch 151 to attribution_results/non-suicide10/layer10_batch151_attrs.json\n",
      "Saved attribution for batch 152 to attribution_results/non-suicide10/layer10_batch152_attrs.json\n",
      "Saved attribution for batch 153 to attribution_results/non-suicide10/layer10_batch153_attrs.json\n",
      "Saved attribution for batch 154 to attribution_results/non-suicide10/layer10_batch154_attrs.json\n",
      "Saved attribution for batch 155 to attribution_results/non-suicide10/layer10_batch155_attrs.json\n",
      "Saved attribution for batch 156 to attribution_results/non-suicide10/layer10_batch156_attrs.json\n",
      "Attribution saving complete.\n"
     ]
    }
   ],
   "source": [
    "#create dir for saving\n",
    "layer_num = 10\n",
    "save_dir = f\"attribution_results/non-suicide{layer_num}\"\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "#limit to 12th layer\n",
    "all_layers = {f'model.bert.encoder.layer.{layer_num}':model.__getattr__('bert').__getattr__('encoder').__getattr__('layer').__getattr__(f'{layer_num}')}\n",
    "\n",
    "layer_names = list(all_layers.keys())\n",
    "\n",
    "layer_act = LayerActivation(model, list(all_layers.values()))\n",
    "for batch_idx, (input_ids, attention_masks) in enumerate(train_dataloader_neg):\n",
    "    \n",
    "    attribution = layer_act.attribute((input_ids, attention_masks))\n",
    "    batch_attributions ={k:v.detach().cpu().tolist() for (k,v) in zip(layer_names , attribution)} \n",
    "    #attribution = [Tensor( batch1_act, batch2_act....)]\n",
    "    # print(len(attribution))\n",
    "    json_obj = json.dumps(batch_attributions, indent=4)\n",
    "    filename = f\"{save_dir}/layer{layer_num}_batch{batch_idx}_attrs.json\"\n",
    "    with open(filename, \"w\") as outfile:\n",
    "        outfile.write(json_obj)\n",
    "\n",
    "     \n",
    "    print(f\"Saved attribution for batch {batch_idx} to {filename}\")\n",
    "    del attribution\n",
    "    del batch_attributions\n",
    "    gc.collect()\n",
    "    \n",
    "print(\"Attribution saving complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved attribution for batch 150 to attribution_results/suicide10/layer10_batch150_attrs.json\n",
      "Saved attribution for batch 151 to attribution_results/suicide10/layer10_batch151_attrs.json\n",
      "Saved attribution for batch 152 to attribution_results/suicide10/layer10_batch152_attrs.json\n",
      "Saved attribution for batch 153 to attribution_results/suicide10/layer10_batch153_attrs.json\n",
      "Saved attribution for batch 154 to attribution_results/suicide10/layer10_batch154_attrs.json\n",
      "Saved attribution for batch 155 to attribution_results/suicide10/layer10_batch155_attrs.json\n",
      "Saved attribution for batch 156 to attribution_results/suicide10/layer10_batch156_attrs.json\n",
      "Attribution saving complete.\n"
     ]
    }
   ],
   "source": [
    "#create dir for saving\n",
    "save_dir = f\"attribution_results/suicide{layer_num}\"\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "#limit to 12th layer\n",
    "all_layers = {f'model.bert.encoder.layer.{layer_num}':model.__getattr__('bert').__getattr__('encoder').__getattr__('layer').__getattr__(f'{layer_num}')}\n",
    "\n",
    "layer_names = list(all_layers.keys())\n",
    "\n",
    "layer_act = LayerActivation(model, list(all_layers.values()))\n",
    "for batch_idx, (input_ids, attention_masks) in enumerate(train_dataloader_pos):\n",
    "    if batch_idx<150:\n",
    "        continue\n",
    "    attribution = layer_act.attribute((input_ids, attention_masks))\n",
    "    batch_attributions ={k:v.detach().cpu().tolist() for (k,v) in zip(layer_names , attribution)} \n",
    "    #attribution = [Tensor( batch1_act, batch2_act....)]\n",
    "    # print(len(attribution))\n",
    "    json_obj = json.dumps(batch_attributions, indent=4)\n",
    "    filename = f\"{save_dir}/layer{layer_num}_batch{batch_idx}_attrs.json\"\n",
    "    with open(filename, \"w\") as outfile:\n",
    "        outfile.write(json_obj)\n",
    "\n",
    "     \n",
    "    print(f\"Saved attribution for batch {batch_idx} to {filename}\")\n",
    "    del attribution\n",
    "    del batch_attributions\n",
    "    gc.collect()\n",
    "print(\"Attribution saving complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[1]\n",
    "sample_tok = bert_tokenizer(X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Am I weird I don't get affected by compliments if it's coming from someone I know irl but I feel really good when internet strangers do it\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0136, -0.0265, -0.0235,  ...,  0.0087,  0.0071,  0.0151],\n",
       "        [-0.0437, -0.0150,  0.0029,  ..., -0.0282,  0.0474, -0.0448],\n",
       "        [-0.0211,  0.0059, -0.0179,  ...,  0.0163,  0.0122,  0.0073],\n",
       "        ...,\n",
       "        [-0.0102, -0.0615, -0.0265,  ..., -0.0199, -0.0372, -0.0098],\n",
       "        [-0.0102, -0.0615, -0.0265,  ..., -0.0199, -0.0372, -0.0098],\n",
       "        [-0.0102, -0.0615, -0.0265,  ..., -0.0199, -0.0372, -0.0098]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_act = LayerActivation(model, model.__getattr__('bert').__getattr__('embeddings').__getattr__('word_embeddings'))\n",
    "attribution = layer_act.attribute(sample_tok)\n",
    "attribution[0][0:500]\n",
    "# model.bert.embeddings.word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3966,  0.0843, -0.1734,  ..., -0.1993,  0.0263,  0.2048],\n",
       "        [-0.0821, -0.0142,  0.2886,  ...,  0.2647,  0.1573, -0.1333],\n",
       "        [-0.2309, -0.0342, -0.1535,  ..., -0.1660, -0.0533,  0.2310],\n",
       "        ...,\n",
       "        [ 0.1632, -0.1154,  0.6387,  ...,  0.2678, -0.0523,  0.1187],\n",
       "        [ 0.3812, -0.0009,  0.6487,  ...,  0.3838, -0.0097, -0.0421],\n",
       "        [-0.0630,  0.0188,  0.2642,  ..., -0.0731, -0.1828,  0.3592]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_act = LayerActivation(model, model.__getattr__('bert').__getattr__('encoder').__getattr__('layer').__getattr__('5'))\n",
    "attribution = layer_act.attribute(sample_tok)\n",
    "attribution[0][350:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1375, -0.8790,  0.1797,  ...,  0.1689,  0.5940, -0.3195],\n",
       "        [-0.0205, -0.7896,  0.1557,  ...,  0.1930,  0.5488, -0.4349],\n",
       "        [-0.0996, -0.8993,  0.1445,  ...,  0.2475,  0.5433, -0.3921],\n",
       "        ...,\n",
       "        [ 0.0565, -0.4912,  0.1698,  ...,  0.1381,  0.5907, -0.2065],\n",
       "        [ 0.0384, -0.2033,  0.2311,  ...,  0.0278,  0.4072, -0.0010],\n",
       "        [ 0.0765, -0.7767,  0.1176,  ...,  0.2635,  0.5953, -0.3277]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_act = LayerActivation(model, model.__getattr__('bert').__getattr__('encoder').__getattr__('layer').__getattr__('11'))\n",
    "attribution = layer_act.attribute(sample_tok)\n",
    "attribution[0][350:500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\salma\\AppData\\Local\\Temp\\ipykernel_16168\\1604208270.py:1: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at ..\\aten\\src\\ATen/native/IndexingUtils.h:28.)\n",
      "  attribution[np.invert(sample_tok[1].bool())]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0226, -0.4424,  0.0795,  ...,  0.1186,  0.6583, -0.2850],\n",
       "        [-0.0712, -0.7913,  0.0977,  ...,  0.2465,  0.6118, -0.3723],\n",
       "        [-0.0239, -0.5132,  0.1971,  ...,  0.1724,  0.6006, -0.2474],\n",
       "        ...,\n",
       "        [-0.2629, -0.6130,  0.2353,  ...,  0.0360,  0.5379, -0.3160],\n",
       "        [-0.2251, -0.8889,  0.1287,  ...,  0.0596,  0.6490, -0.4362],\n",
       "        [-0.1725, -0.5995,  0.2570,  ..., -0.0263,  0.5621, -0.3175]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attribution[np.invert(sample_tok[1].bool())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[1].split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  2572,  1045,  6881,  1045,  2123,  1005,  1056,  2131,  5360,\n",
       "         2011, 19394,  2015,  2065,  2009,  1005,  1055,  2746,  2013,  2619,\n",
       "         1045,  2113, 20868,  2140,  2021,  1045,  2514,  2428,  2204,  2043,\n",
       "         4274, 12358,  2079,  2009,   102,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sample_tok[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "somevarname = bert_tokenizer(X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  101,  2572,  1045,  6881,  1045,  2123,  1005,  1056,  2131,  5360,\n",
       "           2011, 19394,  2015,  2065,  2009,  1005,  1055,  2746,  2013,  2619,\n",
       "           1045,  2113, 20868,  2140,  2021,  1045,  2514,  2428,  2204,  2043,\n",
       "           4274, 12358,  2079,  2009,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " ['[CLS]',\n",
       "  'am',\n",
       "  'i',\n",
       "  'weird',\n",
       "  'i',\n",
       "  'don',\n",
       "  \"'\",\n",
       "  't',\n",
       "  'get',\n",
       "  'affected',\n",
       "  'by',\n",
       "  'compliment',\n",
       "  '##s',\n",
       "  'if',\n",
       "  'it',\n",
       "  \"'\",\n",
       "  's',\n",
       "  'coming',\n",
       "  'from',\n",
       "  'someone',\n",
       "  'i',\n",
       "  'know',\n",
       "  'ir',\n",
       "  '##l',\n",
       "  'but',\n",
       "  'i',\n",
       "  'feel',\n",
       "  'really',\n",
       "  'good',\n",
       "  'when',\n",
       "  'internet',\n",
       "  'strangers',\n",
       "  'do',\n",
       "  'it',\n",
       "  '[SEP]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]'])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "somevarname"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-gpu-10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
